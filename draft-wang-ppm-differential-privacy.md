---
title: "Differential Privacy Mechanisms for DAP"
abbrev: "DP-PPM"
category: info

docname: draft-wang-ppm-differential-privacy-latest
submissiontype: IETF  # also: "independent", "IAB", or "IRTF"
number:
date:
consensus: true
v: 3
area: "Security"
workgroup: "Privacy Preserving Measurement"
keyword:
 - next generation
 - unicorn
 - sparkling distributed ledger
venue:
  group: "Privacy Preserving Measurement"
  type: "Working Group"
  mail: "ppm@ietf.org"
  arch: "https://mailarchive.ietf.org/arch/browse/ppm/"
  github: "wangshan/draft-wang-ppm-differential-privacy"
  latest: "https://wangshan.github.io/draft-wang-ppm-differential-privacy/draft-wang-ppm-differential-privacy.html"

author:
 -
    fullname: Junye Chen
    organization: Apple Inc.
    email: "junyec@apple.com"

    fullname: Christopher Patton
    organization: Cloudflare
    email: "chrispatton+ietf@gmail.com"

    fullname: Shan Wang
    organization: Apple Inc.
    email: "shan_wang@apple.com"


normative:

informative:

  FMT20:
    title: "Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling"
    author:
      - ins: V. Feldman
      - ins: A. McMillan
      - ins: K. Talwar
    date: 2020
    target: https://arxiv.org/abs/2012.12803

  FMT22:
    title: "Stronger Privacy Amplification by Shuffling for Rényi and Approximate Differential Privacy"
    author:
      - ins: V. Feldman
      - ins: A. McMillan
      - ins: K. Talwar
    date: 2022
    target: https://arxiv.org/abs/2208.04591

  MJTBp22:
    title: "Private Federated Statistics in an Interactive Setting"
    author:
      - ins: A. McMillan
      - ins: O. Javidbakht
      - ins: K. Talwar
      - ins: E. Briggs
      - ins: M. Chatzidakis
      - ins: J. Chen
      - ins: J. Duchi
      - ins: V. Feldman
      - ins: Y. Goren
      - ins: M. Hesse
      - ins: V. Jina
      - ins: A. Katti
      - ins: A. Liu
      - ins: C. Lyford
      - ins: J. Meyer
      - ins: A. Palmer
      - ins: D. Park
      - ins: W. Park
      - ins: G. Parsa
      - ins: P. Pelzl
      - ins: R. Rishi
      - ins: C. Song
      - ins: S. Wang
      - ins: S. Zhou
    date: 2022
    target: https://arxiv.org/abs/2211.10082

  Mir17:
    title: "Rényi Differential Privacy"
    author:
      - ins: I. Mironov
    date: 2017
    target: https://arxiv.org/abs/1702.07476

  CKS20:
    title: "The Discrete Gaussian for Differential Privacy"
    author:
      - ins: C. L. Canonne
      - ins: G. Kamath
      - ins: T. Steinke
    date: 2020
    target: https://arxiv.org/abs/2004.00010

  KOV15:
    title: "The Composition Theorem for Differential Privacy"
    author:
      - ins: P. Kairouz
      - ins: S. Oh
      - ins: P. Viswanath
    date: 2015
    target: http://proceedings.mlr.press/v37/kairouz15.pdf

  DMNS06:
    title: "Calibrating Noise to Sensitivity in Private Data Analysis"
    author:
      - ins: C. Dwork
      - ins: F. McSherry
      - ins: K. Nissim
      - ins: A. Smith
    date: 2006
    target: https://link.springer.com/chapter/10.1007/11681878_14

  DR14:
    title: "The Algorithmic Foundations of Differential Privacy"
    author:
      - ins: C. Dwork
      - ins: A. Roth
    date: 2014
    target: https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf

  BS16:
    title: "Concentrated Differential Privacy: Simplifications, Extensions, and Lower Bounds"
    author:
      - ins: M. Bun
      - ins: T. Steinke
    date: 2016
    target: https://arxiv.org/abs/1605.02065


--- abstract

TODO Abstract


--- middle

# Introduction

Multi-party computation systems like the Distributed Aggregration Protocol
{{!DAP=I-D.draft-ietf-ppm-dap-05}} enable secure aggregation of measurements
generated by individuals without handling the measurements in the clear. This
is made possible by using a Verifiable Distributed Aggregation Function
{{!VDAF=I-D.draft-irtf-cfrg-vdaf-06}}, the core cryptographic component of DAP.
Execution of A VDAF involves: a large set of "Clients" who produce
cryptographically protected measurements, called "reports"; a small number of
"Aggregators" who consume reports and produce the cryptographically protected
aggregate; and a "Collector" who consumes the aggregate result. Distributing
the computation of the aggregate in this manner ensures that, as long as one
Aggregator is honest, no attacker can learn an honest Client's measurement.

Depending on the application, protecting the measurements may not be sufficient
for privacy, since the aggregate itself can reveal privacy-sensitive
information. As an illustrative example, consider using DAP/VDAF to summarize
the distribution of the heights of respondents to a survey. If one of the
respondents is especially short or tall, then their contribution is likely to
skew the summary statistic in a way that reveals their height. Ideally, no
individual measurement would have such a signficant impact on the aggregate
result, but in general such leakage is inevitable.

This intuition can be formalized by the notion differential privacy {{DMNS06}}.
Differentially privacy is a property of an algorithm or protocol that computes
some function of a set of measurements. We say the algorithm or protocol is
"differentially private", or "DP", if the probability of observing a particular
output does not change signficantly as a result of removing one of the
measurements (or substituting it with another).

VDAFs are not DP on their own, but they can be composed with a variety of
mechanisms that endow them with this property. All such mechanisms work by
introducing "noise" into the computation that is carefully calibrated for a
number of application-specific parameters, including the structure and number
of measurements and the desired aggregation function. [TODO: Anything else?]

Noise can be introduced at various steps at the computation, and by various
parties. Depending on the mechanism: the Clients might add noise to their own
measurements; the Aggregators might add noise to their aggregate shares (the
values they produce for the Collector); and the Collector might add noise to
the aggregate result before publishing it.

> NOTE The Collector adding noise is mentioned here only as a strawman. We
> typically consider the Collector to be under control of the attacker, so we
> want to ensure the aggregate result is DP before the Collector computes it.

In this document, we shall refer to the composition of DP mechanisms into a
scheme that provides (some notion of) DP as a "DP policy". For some policies,
noise is added only by the Clients or only by the Aggregators, but for others,
noise is added by both Clients and Aggregators. The main advantage of this
strategy is that some degree of DP is achieved even if all other parties, in
particular all Aggregators, are under control of the attacker. On the other
hand, the noise introduced by honest Aggregators amplifies this baseline level
of DP {{FMT22}}.

The primary goal of this document is to specify how DP policies are implemented
in DAP. It does so in the following stages:

1. {{overview}} provides an overview of DP properties in the literature that
   apply to VDAFs in general and DAP/VDAF in particular. Many refinements of
   the basic DP notion are possible, some of which are not possible given the
   constraints imposed by DAP. This section enumerates these constraints and
   describes the baseline DP requirements for DAP.

1. {{mechanisms}} specifies various mechanisms required for building DP
   systems, including algorithms for sampling from discrete Laplace and
   Gaussian distributions.

1. {{policies}} defines DP policies and specifies concrete policies for
   endowing VDAFs in {{!VDAF}} with DP. [TODO: And other drafts, once they
   appear.]

1. {{dp-in-dap}} specifies the integration of DP policies from the previous
   section into DAP. In particular, it describes changes to the Client,
   Aggregator, and Collector behavior required to implement the policy.

The following considerations are out-of-scope for this document:

1. Apart from privacy, the primary consideration for choosing a DP policy and
   calibrating it for an application is "utility". Intuitively, the more noise
   is added, the stronger the DP guarantee, but adding too much noise can
   reduce the veracity of the result. This document provides no guidance for
   this selection process.

1. This document describes a particular class of narrowly-scoped DP policies.
   Other, more sophisticated policies are possible. [TODO: Add citations.]

1. The mechanisms described in {{mechanisms}} are intended for use beyond
   DAP/VDAF. However, this document does not describe general-purpose DP
   policies; those described in {{policies}} are tailored to specific VDAFs.

# Conventions and Definitions

{::boilerplate bcp14-tagged}

This document uses the same conventions for error handling as {{!DAP}}.

> TODO: add more

# Overview of Differential Privacy {#overview}

Differential privacy is a set of techniques used to protect the privacy of
individuals when analyzing user's data. It provides a mathematical framework
that ensures the analysis of a dataset does not reveal identifiable information
about any specific individuals. The advantage of differential privacy is that it
provides a strong, quantifiable and composable privacy guarantee. The main idea
of differential privacy is to add carefully calibrated noise to the results,
which makes it difficult to determine with high certainty whether a specific
individual's data was included in the results or not.

## Differential privacy models

There are two model of differential privacy: deletion differential privacy (
deletion dp) and replacement differntial privacy (replacement dp).

> OPEN ISSUE: or call it substitution dp.

For two dataset X and Y that differ only by 1 data point. Then the difference
between the two are:
* Replacement DP: Y differs from X by replacing a data point in X.
* Deletion DP: Y differs from X by removing a data point in X.

> TODO: make the definition more accurate
> TODO: talk about the relationship of the two


## Differential privacy levels {#levels}

Ther are two levels of privacy protection: local differential privacy (local DP)
and aggregator differential privacy (aggregator DP).

> OPEN ISSUE: or call it secure aggregator dp, or central dp.

In the local-DP settings, Clients apply noise to their own measurements. In
this way, Clients have some control to protect the privacy of their own data.
Any measurement uploaded by a Client will be have local dp, Client's privacy is
protected even if none of the Aggregators is honest (although this protection
may be weak). Furthermore, one can analyze the aggregator DP guarantee with
privacy amplification by aggregation, assuming each Client has added the
required amount of local DP noise, and there are at least minimum batch size
number of Clients in the aggregation.

In Aggregator DP settings, an Aggregator applies noise on the aggregation.
Aggregator DP relies on the server being secure and trustworthy. Aggregators
built using DAP protocol is ideal for this setting because DAP ensures no server
can access any individual data, but only the aggregation.

If there are no local DP added from client, noise added to the aggregation
provides the privacy guarantee of the aggregation.

One can use the Aggregator DP noise together with local DP noise to achieve
privacy guarantee. If the DP guarantee is achieved with a minimum batch size
number of Clients adding local DP noise, and minimum batch size is not reached
when a data collection task expires, each Aggregator can add the remaining noise
by generating the same local DP noise, on the missing number of Clients being
the gap between actual number of Clients and minimum batch size.


## Protected entity

> TODO: Chris P to fill: user or report, given time

## Privacy budget and accounting {#budget}

There are various types of DP guarantees and budgets that can be enforced.
Many applications need to query the Client data multiple times, for example:

* Federated machine learning applications require multiple aggregates to be
  computed over the same underlying data, but with different machine learning
  model parameters.

* {{MJTBp22}} describes an interactive approach of building histograms over
  multiple iterations, and Section 4.3 describes a way to track Client-side
  budget when the Client data is queried multiple times.

> TODO: have citations for machine learning

It’s hard for Aggregator(s) to keep track of the privacy budget over time,
because different Clients can participate in different data collection tasks,
and only Clients know when their data is queried. Therefore, Clients must
enforce the privacy budget.

There could be multiple ways to compose DP guarantees, based on different
DP composition theorems. In the various example DP guarantees below,
we describe the following:

* A formal definition of the DP guarantee.

* Composition theorems that apply to the DP guarantee.

### Pure `EPSILON`-DP, or `(EPSILON, DELTA)`-approximate DP {#adp}

Pure `EPSILON`-DP was first proposed in {{DMNS06}}, and a formal definition of
`(EPSILON, DELTA)`-DP can be found in Definition 2.4 of {{DR14}}.

The `EPSILON` parameter quantifies the "privacy loss" of observing the outcomes
of querying two databases differing by one element. The smaller `EPSILON` is,
the stronger the privacy guarantee is, that is, the outcomes of querying two
adjacent databases are more or less the same.
The `DELTA` parameter provides a small probability of the privacy loss
exceeding `EPSILON`.

One can compose multiple `(EPSILON, DELTA)`-approximate DP guarantees, per
Theorem 3.4 of {{KOV15}}.
One can also compose the guarantees in other types of guarantee first, such as
Rényi DP {{rdp}}, and then convert the composed guarantee to approximate
DP guarantee.

### `(ALPHA, TAU)`-Rényi DP {#rdp}

A formal definition of Rényi DP can be found in Definitions 3 and 4 of
{{Mir17}}.

The intuition behind Rényi-DP is to use `TAU` parameter to measure the
divergence of probability distributions of querying two adjacent databases,
given Rényi order parameter `ALPHA`. The smaller the `TAU` parameter,
the harder it is to distinguish the outputs from querying two adjacent
databases, and thus the stronger the privacy guarantee is.

One can compose multiple Rényi DP guarantees based on Proposition 1 of
{{Mir17}}.
After composition, one can convert the `(ALPHA, TAU)`-Rényi DP guarantee to
`(EPSILON, DELTA)`-approximate DP, per Proposition 12 of {{CKS20}}.

### Zero Concentrated-DP {#zcdp}

A formal definition of zero Concentrated-DP can be found in Definition 1.1
of {{BS16}}.

Zero Concentrated-DP uses different parameters from Rényi-DP, but uses a similar
idea to measure the output distribution divergence of querying two adjacent
databases.

One can compose multiple zCDP guarantees, per Lemma 1.7 of {{BS16}}.

## Sentitity

> TODO: Chris P to fill: sensitivity, l1 vs l2

## Data type and Noise type

Differential Privacy guarantee can only be achieved if data type is applied
with the correct noise type.

> TODO: Junye to fill, mention DAP is expected to ensure the right pair of VDAF and DP mechanism

> TODO: Chris P: we will mention Prio3SumVec because that's what we use to describe aggregator DP with amplification

# DP Mechanisms {#mechanisms}

This section describes various mechanisms required for implementing DP
policies. The algorithms are designed to securely expand a short, uniform
random seed into a sample from a given distribution.

For each mechanism, we expect the noise parameters are computed based on the
DP guarantee that it is supposed to provide.

We also expect DP mechanisms to contain the following functionalities:

* Add noise to a piece of input data (i.e. a measurement or an aggregate share).
  Some DP mechanisms apply noise based on the input data, e.g. randomized
  response mechanism {{rr}} flips the bit at each dimension, which means the
  noised output depends on the input.

* Sample noise with the DP mechanism.

* Debias the noised data. Note that not all noise will need this functionality.
  Some DP mechanisms will need this functionality, for example, randomized
  response mechanisms have a debiasing step that removes bias.

Therefore, we define three methods for an interface `DpMechanism`:

~~~
class DpMechanism:
    DataType
    DebiasedDataType

    def add_noise(self, data: DataType) -> DataType:
        """Add noise to a piece of input data. """
        pass

    def sample_noise(self, num_reps: int, dimension: int) -> DataType:
        """
        Sample noise for `num_reps` number of times,
        with `dimension`.
        """
        pass

    def debias(self,
               agg: DataType,
               meas_count: int) -> DebiasedDataType:
        """
        Debias the data due to the added noise.
        This doesn't apply to all noises. Some Client-DP mechanisms
        need this functionality.
        """
        pass
~~~

## Discrete Laplace

> TODO: Specify a Laplace sampler

## Discrete Gaussian {#discrete-gaussian}

> TODO: Specify a Gaussian sampler

## Randomized Response {#rr}

> TODO: Specify any mechanisms required for randomized response mechanisms

# DP Policies for VDAFs {#policies}

The section defines a generic interface for DP policies for VDAFs.

We will define an interface `DpPolicy` that composes the following:

* An optional Client-DP mechanism that adds noise to Clients' measurements.

* An optional Aggregator-DP mechanism that adds noise to an Aggregator's
  aggregate share, based on the number of measurements, and the minimum
  batch size.

* An optional debiasing step that removes the bias in DP mechanisms (i.e.
  `DpMechanism.debias`).

The composition of Client- and Aggregator-DP mechanisms defines the DP
policy for a VDAF, and enforces the DP guarantee.

~~~
class DpPolicy:
    Measurement
    AggregateShare
    AggregateResult
    DebiasedAggregateResult

    def add_noise_to_measurement(self,
                                 meas: Measurement,
                                 ) -> Measurement:
        """
        Add noise to measurement, if required by the Client-DP
        mechanism.
        """
        pass

    def add_noise_to_agg_share(self,
                               agg_share: AggregateShare,
                               meas_count: Unsigned,
                               min_batch_size: Unsigned,
                               ) -> AggregateShare:
        """
        Add noise to aggregate share, if required by the Aggregator-DP
        mechanism.
        """
        pass

    def debias_agg_result(self,
                          agg_result: AggregateResult,
                          meas_count: Unsigned,
                          min_batch_size: Unsigned,
                          num_aggregators: Unsigned,
                          ) -> DebiasedAggregateResult:
        """
        Debias aggregate result, if any of the Client- or
        Aggregator-DP mechanism requires this operation,
        based on the number of measurements, minimum batch size,
        and the number of Aggregators.
        """
        pass
~~~

## Concrete Instantiations of DP Policies with VDAFs

### Prio3SumVec, `bits = 1`, with Randomized Response Client-DP

Client-DP allows Clients to protect their privacy by adding noise to their
measurements directly, as described in {{levels}}. Analyses ({{FMT20}} and
{{FMT22}}) have shown that the central DP guarantee can also be amplified
by aggregating Clients' measurements with Client-DP.

For this particular instantiation of `Prio3SumVec` with `bits = 1`, i.e. each
value in the vector is either a 0 or 1, we can apply randomized response {{rr}}
Client-DP to Clients' measurements, with a configured Client-DP with `EPSILON_0`
parameter.

The DP guarantee is met, as long as there are at least "minimum batch size"
number of Clients, each of which adds the randomized response Client-DP.
However, in case the minimum batch size is not met when Aggregators want to
output their aggregate shares, each Aggregator can sample the same randomized
response Client-DP on the "missing" Clients with zero-hot vectors, so the
central DP guarantee can still be satisfied, based on the privacy amplification
analysis in {{FMT20}} and {{FMT22}}.

~~~
class Prio3SumVecWithRandomizedResponse:
    Measurement = Vec[Unsigned]
    AggregateShare = Vec[Field]
    AggregateResult = Vec[Unsigned]
    DebiasedAggregateResult = Vec[float]

    def __init__(self,
                 rr_mechanism: DpMechanism):
        self.rr_mechanism = rr_mechanism

    def add_noise_to_measurement(self,
                                 meas: Measurement,
                                 ) -> Measurement:
        """
        Apply randomized response mechanism to measurement.
        """
        return self.rr_mechanism.add_noise(meas)

    def add_noise_to_agg_share(self,
                               agg_share: AggregateShare,
                               meas_count: Unsigned,
                               min_batch_size: Unsigned,
                               ) -> AggregateShare:
        """
        If minimum batch size is met, this function does nothing,
        because DP guarantee would have already been met.
        Otherwise, add the same randomized response noise on the
        missing Clients.
        """
        if meas_count >= batch_size:
            return agg_share
        noise = self.rr_mechanism.sample_noise(
            batch_size - meas_count, len(agg_share)
        )
        return [a + Field(b) for (a, b) in zip(agg_share, noise)]

    def debias_agg_result(self,
                          agg_result: AggregateResult,
                          meas_count: Unsigned,
                          min_batch_size: Unsigned,
                          num_aggregators: Unsigned,
                          ) -> DebiasedAggregateResult:
        """
        Debias aggregate result with randomized response.
        """
        if meas_count >= min_batch_size:
            return self.rr_mechanism.debias(agg_result, meas_count)
        # The count passed to debiasing will be:
        # the number of actual Clients + the "missing" Clients
        # sampled by all Aggregators.
        debiasing_count = \
            meas_count + \
            num_aggregators * (min_batch_size - meas_count)
        return self.rr_mechanism.debias(agg_result, debiasing_count)
~~~

> TODO(issue #10): replace `rr_mechanism` once we have concretely defined
> it in {{rr}}.

### Prio3Histogram with Discrete Gaussian

For Prio3Histogram, we will apply an Aggregator-only DP mechanism,
that is implemented with discrete Gaussian.

~~~
class Prio3HistogramWithDiscreteGaussian:
    Measurement = Unsigned
    AggregateShare = Vec[Field]
    AggregateResult = Vec[int]
    DebiasedAggregateResult = AggregateResult

    def __init__(self,
                 dgauss_mechanism: DpMechanism,
                 ):
        self.dgauss_mechanism = dgauss_mechanism

    def add_noise_to_measurement(self,
                                 meas: Measurement,
                                 ) -> Measurement:
        """
        No Client-DP here.
        """
        return meas

    def add_noise_to_agg_share(self,
                               agg_share: AggregateShare,
                               meas_count: Unsigned,
                               min_batch_size: Unsigned,
                               ) -> AggregateShare:
        """
        Sample discrete Gaussian noise, and merge it with
        aggregate share.
        """
        # Sample the noise once, with length equal to the length
        # of aggregate share.
        noise_vec = self.dgauss_mechanism.sample_noise(
            1, len(agg_share)
        )
        result = []
        for i in range(len(agg_share)):
            noise = noise_vec[i]
            if noise < 0:
                noise = Field.MODULUS + noise
            result.append(agg_share[i] + Field(noise))
        return result

    def debias_agg_result(self,
                          agg_result: AggregateResult,
                          meas_count: Unsigned,
                          min_batch_size: Unsigned,
                          num_aggregators: Unsigned,
                          ) -> DebiasedAggregateResult:
        """
        No debiasing.
        """
        return agg_result
~~~

> TODO(issue #10): replace `dgauss_mechanism` once we have concretely defined
> it in {{discrete-gaussian}}.

# Executing DP Policies in DAP {#dp-in-dap}

> TODO: Specify integration of a `DpPolicy` into DAP.

# Security Considerations

TODO Security


# IANA Considerations

This document has no IANA actions.


--- back

# Acknowledgments
{:numbered="false"}

TODO acknowledge.
